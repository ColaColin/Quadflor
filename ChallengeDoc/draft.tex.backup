\documentclass{article}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{arrows}

\tikzset{
  treenode/.style = {align=center, inner sep=0pt, text centered,
    font=\sffamily},
  arn_n/.style = {treenode, circle, white, font=\sffamily\bfseries, draw=black,
    fill=black, text width=1.5em},% arbre rouge noir, noeud noir
  defc/.style = {treenode, circle, black, draw=black, 
    text width=2.5em, very thick},% arbre rouge noir, noeud rouge
  arn_x/.style = {treenode, rectangle, draw=black,
    minimum width=0.5em, minimum height=0.5em}% arbre rouge noir, nil
  
}

\begin{document}


\section*{Draft}
Group DMML05: Nils Ziermann, Bjarne Gau, Colin Clausen
\\
We have decided to do the following things:
\\

\begin{itemize}
\item Ensemble of existing methods.
\\
We will create various ensembles from existing classifiers and experiment with leaving out 
single classifiers to measure their effect on the performance of the overall ensemble.
The reason for trying this is because ensembles typically deliver better performance than single classifiers.

\item SELU activated neural networks
\\
We will experiment with neural networks that use the SELU activation function (see https://arxiv.org/pdf/1706.02515.pdf). These promise to improve the performance of simple feeed forward neural nets without the use of convolutions or batch normalization.

\item Random Search of Hyperparamters
\\
We will search for good hyperparameters in our experiments by randomly sampling hyperparameter configurations from reasonable ranges and evaluating them. This will be limited by our available computational power. This approach prevents us from having to come up with good hyperparameters for learning algorithmns that we have never seen before.

\end{itemize}

\section*{First results}

Our initial ideas were based on the example dataset that was very limited in size. This gave us the impression would could try a lot more computationally expensive methods than we actually could.
The main problem was not the number of input documents, but the size of the input vocabulary. For the example dataset this was roughly 35000, which seemed managable. However this size turned out to be not static and grew
with the dataset. The complete dataset uses a vocabulary of roughly 1.3 million words. This meant we had to adapt our suggested approaches.

\subsubsection*{Ensemblemethods}

TODO

\subsection*{SELU activated neural networks}

The most simple type of neural network, the multilayer perceptron shown in Figure \ref{fig:mlp}, uses fully connected layers. In these each unit is connected to all outputs of the previous layer. For the first layer this is the 
input of the network. For the complete dataset this poses a problem, as the size of the input exceeds 1.3 million datapoints. This means that first layer of the network would have 1.3 million weighted connections to each 
unit in the first layer from the input alone. Tests on the smaller dataset suggested a size of roughly 1000 units in the first layer would be a good idea, which however proved problematic to test within acceptable time, as 
the resulting network had nearly 1 billion parameters. The resulting network filled a large quantity of ram and a single fold took multiple days to complete on the server we were provided, 
putting over 12 cpu cores under full load.

Since the final results were intended to be tested with 10 fold cross validation this made proceeding further with this kind of network impossible. To proceed further with neural nets two issues had to be solved:

\begin{itemize}
 \item The required computational power exceeds that of a CPU based server, a GPU based system promised speedups of 10 to 30 times.
 \item The networks memory requirements needed were too large to use the network on anything but the server with its immense amounts of system memory. No graphics card has the dozens of gigabytes the training run of the 1
 billion parameter network required.
\end{itemize}

The solution to these two problems was found in the use of the LocallyConnected1D layer of the Keras framework, shown in \ref{fig:locallyconnected}. This is a layer that, similar to the convolutional layers, applies 
units that only have weighted connections to a few inputs at a time. However instead of the normal convolutional layers the weights are not shared. This effectively means the layers behave like fully 
connected layers with most weights cut out of them. By using locally connected units that are connected to 1000 input values each and are spaced out with a stride of 500 units the amount of parameters
could be drastically reduced. Similar to convolutional layers the LocallyConnected1D layer also supports multiple feature layers all connected to the same output.
This produces a 2D output that is then flattened again to be connected to normal fully connected layers.
Preliminary tests showed no loss of classification performance, most likely since the input vectors are very sparse and high level connections between inputs far away from each other can be made 
in later layers of the network.

With the size of the network reduced to managable levels it became possible to run tests on a GPU instead of the CPU server, which speed up the experiments by nearly an order of magnitude.

\subsubsection*{Results}

The code used to created these results can be found on github: \url{https://github.com/ColaColin/Quadflor/blob/master/Code/lucid_ml/classifying/selu_net.py}.
Training was done using early stopping based on using 9\% of the training data of each fold as validation data. The validation criterion was the sample f1 score as calculated by sklearn.
After 6 epochs of no improvements the learning rate was reduced by a factor of 10, after 9 epochs of no improvements trainings was considered to be complete. This typically happened after 80 to 120 
epochs. Preliminary 1 fold experiments have shown that deeper networks seem to not be very successful on this dataset. This suggests there may not be a 
big advantage of using the SELU activation function, as it is especially intended for deeper networks. The full 10 fold cross validation runs all were done
with 2 extra hidden layers after the locally connected layer, so 3 hidden layers overall, as this seemed to provide good results.

\begin{table} [H]
 \centering
  \begin{tabular}{l | r}
   Network [10, 2048, 2048] &  \\
   \hline
   \hline
   avg n labels gold & 5.240 $\pm 0.015$ \\
   \hline
   avg n labels pred & 5.410 $\pm 0.245$ \\
   \hline
   f1 macro & 0.223 $\pm 0.006$ \\
   \hline
   f1 micro & 0.505 $\pm 0.009$ \\
   \hline
   f1 samples & 0.508 $\pm 0.005$ \\
   \hline
   p macro & 0.245 $\pm 0.012$ \\
   \hline
   p micro & 0.497 $\pm 0.017$ \\
   \hline
   p samples & 0.555 $\pm 0.008$ \\
   \hline
   r macro & 0.231 $\pm 0.005$ \\
   \hline
   r micro & 0.531 $\pm 0.009$ \\
   \hline
   r samples & 0.525 $\pm 0.009$ \\
   \hline
   10 fold cross validation run time & 36 hours \\
   \hline
   network size & 95.2 million parameters \\
  \end{tabular}

  \begin{tabular}{l | r}
   Network [3, 1024, 1024] &  \\
   \hline
   \hline
   avg n labels gold & 5.240 $\pm 0.022$ \\
   \hline
   avg n labels pred & 5.410 $\pm 0.245$ \\
   \hline
   f1 macro & 0.223 $\pm 0.006$ \\
   \hline
   f1 micro & 0.505 $\pm 0.009$ \\
   \hline
   f1 samples & 0.508 $\pm 0.005$ \\
   \hline
   p macro & 0.245 $\pm 0.012$ \\
   \hline
   p micro & 0.497 $\pm 0.017$ \\
   \hline
   p samples & 0.555 $\pm 0.008$ \\
   \hline
   r macro & 0.231 $\pm 0.005$ \\
   \hline
   r micro & 0.531 $\pm 0.009$ \\
   \hline
   r samples & 0.525 $\pm 0.009$ \\
   \hline
   10 fold cross validation run time & 21 hours \\
   \hline
   network size & 22 million \\
  \end{tabular}

  \begin{tabular}{l | r}
   Network [2, 512, 512] &  \\
   \hline
   \hline
   avg n labels gold & 5.240 $\pm $ \\
   \hline
   avg n labels pred & $\pm $ \\
   \hline
   f1 macro & $\pm $ \\
   \hline
   f1 micro &  $\pm $ \\
   \hline
   f1 samples &  $\pm $ \\
   \hline
   p macro &  $\pm $ \\
   \hline
   p micro &  $\pm $ \\
   \hline
   p samples & $\pm $ \\
   \hline
   r macro &  $\pm $ \\
   \hline
   r micro & $\pm $ \\
   \hline
   r samples &  $\pm $ \\
   \hline
   10 fold cross validation run time &  \\
   \hline
   network size & 10.7 million \\
  \end{tabular}
  
  \caption{Results of SELU networks. The first number of the network descriptions shows the number of feature layers of the locally connected layers}
  \label{fig:selu_results}
\end{table}

These values seem to be able to compete with the standard settings of the included classifiers, like sgd, however it is unclear to us how good they actuall are, as we cannot find the baseline for
this specific dataset.


\def\layersep{2.5cm}
\begin{figure} [H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
      \tikzstyle{every pin edge}=[<-,shorten <=1pt]
      \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
      \tikzstyle{input neuron}=[neuron, fill=green!50];
      \tikzstyle{output neuron}=[neuron, fill=red!50];
      \tikzstyle{hidden neuron}=[neuron, fill=blue!50];
      \tikzstyle{annot} = [text width=4em, text centered]

      % Draw the input layer nodes
      \foreach \name / \y in {1,...,4}
      % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
	  \node[input neuron, pin=left:Input \#\y] (I-\name) at (0,-\y) {};

      % Draw the hidden layer nodes
      \foreach \name / \y in {1,...,5}
	  \path[yshift=0.5cm]
	      node[hidden neuron] (H-\name) at (\layersep,-\y cm) {};

	      
      \foreach \name / \y in {1,...,3}
	  \path[yshift=0.5cm]
	      node[output neuron,pin={[pin edge={->}]right:Ausgabe \#\y}] (O-\name) at (\layersep * 2,-\y - 1) {};

      % Connect every node in the input layer with every node in the
      % hidden layer.
      \foreach \source in {1,...,4}
	  \foreach \dest in {1,...,5}
	      \path (I-\source) edge (H-\dest);

	      
      \foreach \source in {1,...,5}
	  \foreach \dest in {1,...,3}
	      \path (H-\source) edge (O-\dest);

      % Annotate the layers
      \node[annot,above of=H-1, node distance=1cm] (hl) {Hidden Layer};
      \node[annot,left of=hl] {Input Layer};
      \node[annot,right of=hl] {Output Layer};
  \end{tikzpicture}
  \caption{A normal multi layer perceptron.}
  \label{fig:mlp}
\end{figure}


\def\layersep{2.5cm}
\begin{figure} [H]
  \centering
  \begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
      \tikzstyle{every pin edge}=[<-,shorten <=1pt]
      \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
      \tikzstyle{input neuron}=[neuron, fill=green!50];
      \tikzstyle{output neuron}=[neuron, fill=red!50];
      \tikzstyle{hidden neuron}=[neuron, fill=blue!50];
      \tikzstyle{foo neuron}=[neuron, fill=yellow!50];
      \tikzstyle{annot} = [text width=12em, text centered]

      % Draw the input layer nodes
      \foreach \name / \y in {1,...,8}
      % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
	  \node[input neuron, pin=left:Input \#\y] (I-\name) at (0,-\y) {};

      % Draw the hidden layer nodes
      \foreach \name / \y in {1,...,3}
	  \path[yshift=-1.5cm]
	      node[hidden neuron] (H-\name) at (\layersep,-\y cm) {};

      % Draw the hidden layer nodes
      \foreach \name / \y in {4,...,6}
	  \path[yshift=-1.5cm]
	      node[foo neuron] (H-\name) at (\layersep,-\y cm) {};
	      
	      
      \foreach \name / \y in {1,...,3}
	  \path[yshift=-1.5cm]
	      node[output neuron,pin={[pin edge={->}]right:Rest of network \#\y}] (O-\name) at (\layersep * 2,-\y - 1) {};
      
      \path (I-1) edge (H-1);
      \path (I-2) edge (H-1);
      \path (I-3) edge (H-1);
      \path (I-4) edge (H-1);
      
      \path (I-3) edge (H-2);
      \path (I-4) edge (H-2);
      \path (I-5) edge (H-2);
      \path (I-6) edge (H-2);
      
      \path (I-5) edge (H-3);
      \path (I-6) edge (H-3);
      \path (I-7) edge (H-3);
      \path (I-8) edge (H-3);
      
      \path (I-1) edge (H-4);
      \path (I-2) edge (H-4);
      \path (I-3) edge (H-4);
      \path (I-4) edge (H-4);
      
      \path (I-3) edge (H-5);
      \path (I-4) edge (H-5);
      \path (I-5) edge (H-5);
      \path (I-6) edge (H-5);
      
      \path (I-5) edge (H-6);
      \path (I-6) edge (H-6);
      \path (I-7) edge (H-6);
      \path (I-8) edge (H-6);
      
      \foreach \source in {1,...,6}
	  \foreach \dest in {1,...,3}
	      \path (H-\source) edge (O-\dest);

      % Annotate the layers
      \node[annot,above of=H-1, node distance=2cm] {Locally Connected Layer};
      
  \end{tikzpicture}
  \caption{A simplified example of the locally connected layer. In the real code each hidden units is connected with 1000 input units at a stride of 500. This example connects each 
  hidden units to 4 units at a stride of 2. In the real code a padding with zeros is used to make the first layer fit in. Notice that the locally connected layer in this example
  uses 2 feature layers, displayed in blue and yellow, that are all computed in parallel, similar to the feature layers in typical 2d convolutions, just in 1d.}
  \label{fig:locallyconnected}
\end{figure}


\subsection*{Random Search of Hyperparameters}
The unexpectedly large number of vocabulary, especially on the complete dataset, has prevented us from doing any kind of random search. We've had to resort to guess a few promising values and try those.
More computational power would be required to do a full random search.



\end{document}