\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Results of SELU networks. The first number of the network descriptions shows the number of feature layers of the locally connected layers. It can be seen that overall the middle sized network 3,1024,1024 performs best.}}{4}{table.1}}
\newlabel{fig:selu_results}{{1}{4}{Results of SELU networks. The first number of the network descriptions shows the number of feature layers of the locally connected layers. It can be seen that overall the middle sized network 3,1024,1024 performs best}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A normal multi layer perceptron.}}{5}{figure.1}}
\newlabel{fig:mlp}{{1}{5}{A normal multi layer perceptron}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A simplified example of the locally connected layer. In the real code each hidden units is connected with 1000 input units at a stride of 500. This example connects each hidden units to 4 units at a stride of 2. In the real code a padding with zeros is used to make the first layer fit in. Notice that the locally connected layer in this example uses 2 feature layers, displayed in blue and yellow, that are all computed in parallel, similar to the feature layers in typical 2d convolutions, just in 1d.}}{6}{figure.2}}
\newlabel{fig:locallyconnected}{{2}{6}{A simplified example of the locally connected layer. In the real code each hidden units is connected with 1000 input units at a stride of 500. This example connects each hidden units to 4 units at a stride of 2. In the real code a padding with zeros is used to make the first layer fit in. Notice that the locally connected layer in this example uses 2 feature layers, displayed in blue and yellow, that are all computed in parallel, similar to the feature layers in typical 2d convolutions, just in 1d}{figure.2}{}}
